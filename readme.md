# Сравнение моделей детекции людей: RDETR-L vs YOLO11x

## Установка и использование

### Требования
- Python 3.11 или выше
- [`uv`](https://github.com/astral-sh/uv)

### Установка

1. Клонируйте репозиторий:
```bash
git clone <repository-url>
cd model_comprasion
```

2. Установите `uv` (если еще не установлен):
```bash
pip install uv
```

3. Установите зависимости:
```bash
uv sync
```

### Использование

Обработка видео с помощью YOLO:
```bash
uv run main.py yolo path/to/input.mp4 -o output_yolo.mp4
```

Обработка видео с помощью RT-DETR:
```bash
uv run main.py rtdetr path/to/input.mp4 -o output_rtdetr.mp4
```

Дополнительные параметры:
```bash
uv run main.py yolo input.mp4 --conf 0.3 --imgsz 1080 1920
uv run main.py rtdetr input.mp4 --conf 0.2 --iou 0.4
```

**Примечание:** Если вы используете `pip` вместо `uv`, замените `uv run` на `python`:
```bash
pip install -e .
python main.py yolo path/to/input.mp4 -o output_yolo.mp4
```

Модели будут автоматически загружены:
- YOLO: `yolo11x.pt`
- RT-DETR: `rtdetr-l.pt`

### Разработка

Проверка кода:
```bash
uv run ruff check .
uv run ruff format .
```

Или с использованием `pip`:
```bash
pip install -e .
ruff check .
ruff format .
```

## 1. Используемые модели
- **RDETR-L** — чисто трансформерная архитектура, ориентирована на глобальное восприятие сцены.  
- **YOLO11x** — сверточная архитектура, оптимизированная под быстрый детект объектов с точными масками.

## 2. Эксперименты
- Входное видео обрабатывалось обеими моделями.  
- Итоговые результаты оценивались визуально по качеству масок и по способности детектировать людей на разных расстояниях.

## 3. Текстовый отчет

### 3.1 Выводы по сравнению производительности и качества
- **YOLO11x** показывает более плавные, точные и стабильные маски.  
  **Причина:** сверточная архитектура хорошо работает с локальными признаками, что обеспечивает точное выделение границ объектов на кадре и устойчивость к шуму.  
- **RDETR-L** лучше детектирует людей на большом расстоянии или при плотной толпе.  
  **Причина:** трансформерная архитектура учитывает глобальные связи на разных уровнях, благодаря слоям с различными уровнями абстракции, что позволяет захватывать мелкие и отдаленные объекты.

### 3.2 Обоснование выбора наиболее предпочтительного алгоритма
- Выбор зависит от **целей и способа обработки**:
  - Для задач **bounding boxes**, например, трекинга или online-обработки на видеопотоке, предпочтительнее **YOLO11x** — быстрый, стабильный, точный.  
  - Для задач **people counting** или анализа всей сцены, особенно с отдаленными объектами, предпочтительнее **RDETR-L** — лучше улавливает мелкие детали и дальние объекты.

### 3.3 Шаги по дальнейшему улучшению
- Поиск и использование более подходящих датасетов с плотными толпами и разнообразными сценами для дообучения моделей.  ия inference для edge-устройств, уменьшение латентности при сохранении способности детектировать дальние объекты.  
- Использование методов аугментации данных (rotation, scaling, brightness) для повышения устойчивости моделей.  
- Гибридные архитектуры, объединяющие сильные стороны трансформеров и сверточных сетей.  
- Настройка порогов уверенности (conf) и NMS (iou) для более точной фильтрации детекций.  
- Применение постобработки для сглаживания масок и уменьшения шумов.  
- Дообучение модели (при наличии датасета).
---

## 4. Итог
- **YOLO11x** — лучше для локальных и стабильных масок (bounding boxes, трекинг).  
- **RDETR-L** — лучше для глобальной аналитики сцены (people counting, сцены с дальними объектами).  
- Выбор архитектуры всегда зависит от задачи и условий обработки (offline/cloud/edge).
